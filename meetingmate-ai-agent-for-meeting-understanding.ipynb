{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f138ea4",
   "metadata": {
    "papermill": {
     "duration": 0.004982,
     "end_time": "2025-04-20T04:03:57.939742",
     "exception": false,
     "start_time": "2025-04-20T04:03:57.934760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Capstone Project - MeetingMate: AI Agent for Meeting Understanding\n",
    "\n",
    "Welcome to the capstone project of the Google Generative AI Intensive Course!  \n",
    "In this notebook, we build **MeetingMate**, an intelligent assistant that processes and understands meeting content using multimodal GenAI techniques.\n",
    "\n",
    "We combine various tools such as Whisper for transcription, the Gemini API for content generation, and PowerPoint automation to create presentation-ready summaries. This notebook also introduces semantic retrieval with ChromaDB for contextual grounding.\n",
    "\n",
    "---\n",
    "## Key Features Demonstrated\n",
    "\n",
    "- **Audio Understanding**: Transcribe meeting audio recordings using OpenAI's Whisper model.\n",
    "- **Document Understanding**: Generate structured slide presentations using `python-pptx`.\n",
    "- **Prompting**: Craft prompts for summarization, slide generation, and role-based insights.\n",
    "- **Structured Output / JSON Mode**: Utilize `google.generativeai.types` to format and parse model outputs.\n",
    "- **GenAI Evaluation**: Define rubric-based feedback using prompts to review generated outputs.\n",
    "- **Embeddings & Retrieval-Augmented Generation (RAG)**: Use ChromaDB to store and retrieve prior meeting data based on semantic similarity.\n",
    "\n",
    "---\n",
    "## What We Built\n",
    "\n",
    "MeetingMate takes in a meeting audio file and walks through the following pipeline:\n",
    "\n",
    "1. **Transcription**: Converts meeting audio to text via Whisper.\n",
    "2. **Summarization**: Uses the Gemini model to summarize the transcript.\n",
    "3. **Slide Generation**: Automatically creates a PowerPoint deck summarizing key takeaways.\n",
    "4. **Retrieval**: Retrieves relevant past meeting segments using ChromaDB and embeddings.\n",
    "5. **Evaluation**: Grades the output using structured GenAI-based rubrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- [Whisper](https://github.com/openai/whisper) for audio transcription\n",
    "- [Gemini API](https://ai.google.dev/) for text generation\n",
    "- [python-pptx](https://python-pptx.readthedocs.io/) for slide creation\n",
    "- [ChromaDB](https://docs.trychroma.com/) for semantic search\n",
    "\n",
    "---\n",
    "\n",
    "## Let’s Get Started!\n",
    "\n",
    "Follow the notebook cells sequentially to build and customize your MeetingMate assistant.\n",
    "Make sure your API keys are configured and all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04223949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:03:57.949337Z",
     "iopub.status.busy": "2025-04-20T04:03:57.949008Z",
     "iopub.status.idle": "2025-04-20T04:06:04.982829Z",
     "shell.execute_reply": "2025-04-20T04:06:04.981708Z"
    },
    "papermill": {
     "duration": 127.040562,
     "end_time": "2025-04-20T04:06:04.984564",
     "exception": false,
     "start_time": "2025-04-20T04:03:57.944002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/openai/whisper.git python-pptx google-generativeai\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddb886d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:05.040677Z",
     "iopub.status.busy": "2025-04-20T04:06:05.040298Z",
     "iopub.status.idle": "2025-04-20T04:06:11.741376Z",
     "shell.execute_reply": "2025-04-20T04:06:11.740510Z"
    },
    "papermill": {
     "duration": 6.731098,
     "end_time": "2025-04-20T04:06:11.743017",
     "exception": false,
     "start_time": "2025-04-20T04:06:05.011919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from google import genai\n",
    "\n",
    "import whisper\n",
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from google.genai import types, Client\n",
    "from google.api_core import retry\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "import enum\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c7ab6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:11.800007Z",
     "iopub.status.busy": "2025-04-20T04:06:11.799705Z",
     "iopub.status.idle": "2025-04-20T04:06:11.804665Z",
     "shell.execute_reply": "2025-04-20T04:06:11.803673Z"
    },
    "papermill": {
     "duration": 0.035507,
     "end_time": "2025-04-20T04:06:11.806039",
     "exception": false,
     "start_time": "2025-04-20T04:06:11.770532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c488d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:11.861360Z",
     "iopub.status.busy": "2025-04-20T04:06:11.861041Z",
     "iopub.status.idle": "2025-04-20T04:06:11.866837Z",
     "shell.execute_reply": "2025-04-20T04:06:11.866008Z"
    },
    "papermill": {
     "duration": 0.03509,
     "end_time": "2025-04-20T04:06:11.868190",
     "exception": false,
     "start_time": "2025-04-20T04:06:11.833100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Constants\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "CHROMA_STORAGE_PATH = Path(\"./chroma_storage\")\n",
    "COLLECTION_NAME = \"meeting_summary_collection\"\n",
    "\n",
    "PPTX_FILENAME = \"Meeting_Summary.pptx\"\n",
    "MEETING_THEMES = {\n",
    "    \"team_sync\": {\"title\": \"Team Sync\", \"color\": \"#27AE60\"},\n",
    "    \"project_kickoff\": {\"title\": \"Project Kickoff\", \"color\": \"#2E86C1\"},\n",
    "    \"retrospective\": {\"title\": \"Sprint Retrospective\", \"color\": \"#E67E22\"},\n",
    "    \"client_review\": {\"title\": \"Client Review\", \"color\": \"#8E44AD\"},\n",
    "    \"default\": {\"title\": \"Meeting Summary\", \"color\": \"#34495E\"},\n",
    "}\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 10000\n",
    "TEMPERATURE = 0.4\n",
    "TOP_P = 0.9\n",
    "TOP_K = 40\n",
    "EMBEDDING_TASK = \"retrieval_document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d76a261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:11.924296Z",
     "iopub.status.busy": "2025-04-20T04:06:11.923771Z",
     "iopub.status.idle": "2025-04-20T04:06:11.928433Z",
     "shell.execute_reply": "2025-04-20T04:06:11.927680Z"
    },
    "papermill": {
     "duration": 0.034172,
     "end_time": "2025-04-20T04:06:11.929854",
     "exception": false,
     "start_time": "2025-04-20T04:06:11.895682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# Suppress httpx and google_genai.models INFO logs by default\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"google_genai.models\").setLevel(logging.WARNING)\n",
    "\n",
    "# Configure root logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de20e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:11.987602Z",
     "iopub.status.busy": "2025-04-20T04:06:11.987334Z",
     "iopub.status.idle": "2025-04-20T04:06:12.258365Z",
     "shell.execute_reply": "2025-04-20T04:06:12.257386Z"
    },
    "papermill": {
     "duration": 0.302232,
     "end_time": "2025-04-20T04:06:12.260107",
     "exception": false,
     "start_time": "2025-04-20T04:06:11.957875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set api key\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d66a5c",
   "metadata": {
    "papermill": {
     "duration": 0.097905,
     "end_time": "2025-04-20T04:06:12.389219",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.291314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload Audio File\n",
    "\n",
    "This utility saves uploaded audio content to a temporary `.wav` file \n",
    "for processing by the transcription model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63434abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:12.458906Z",
     "iopub.status.busy": "2025-04-20T04:06:12.457530Z",
     "iopub.status.idle": "2025-04-20T04:06:12.469309Z",
     "shell.execute_reply": "2025-04-20T04:06:12.468084Z"
    },
    "papermill": {
     "duration": 0.048976,
     "end_time": "2025-04-20T04:06:12.471279",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.422303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio file: /kaggle/input/audio-input/spkr0.wav\n"
     ]
    }
   ],
   "source": [
    "audio_dir = Path('/kaggle/input')\n",
    "wav_files = [f for f in audio_dir.rglob('*') if f.suffix in ['.wav', '.mp3']]\n",
    "if not wav_files:\n",
    "    raise FileNotFoundError(\"No .wav files found in /kaggle/input\")\n",
    "# Consider the first audio file\n",
    "src_path = wav_files[0]\n",
    "print(\"Using audio file:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e99b74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:12.528405Z",
     "iopub.status.busy": "2025-04-20T04:06:12.528094Z",
     "iopub.status.idle": "2025-04-20T04:06:12.532935Z",
     "shell.execute_reply": "2025-04-20T04:06:12.532027Z"
    },
    "papermill": {
     "duration": 0.035016,
     "end_time": "2025-04-20T04:06:12.534298",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.499282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save audio file\n",
    "def save_file(content):\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav') \n",
    "    temp_file.write(content)\n",
    "    temp_file.close()\n",
    "    return Path(temp_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687afea",
   "metadata": {
    "papermill": {
     "duration": 0.027617,
     "end_time": "2025-04-20T04:06:12.589962",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.562345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Audio Transcription (Whisper)\n",
    "\n",
    "This class loads a Whisper model and provides transcription functionality.  \n",
    "It is used to convert spoken meeting content into raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf82555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:12.646609Z",
     "iopub.status.busy": "2025-04-20T04:06:12.646309Z",
     "iopub.status.idle": "2025-04-20T04:06:12.651818Z",
     "shell.execute_reply": "2025-04-20T04:06:12.651129Z"
    },
    "papermill": {
     "duration": 0.035093,
     "end_time": "2025-04-20T04:06:12.652992",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.617899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    \"\"\"\n",
    "    Transcribe speech to text using Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name = \"base\"):\n",
    "        try:\n",
    "            self.model = whisper.load_model(model_name)\n",
    "            logger.info(f\"Loaded Whisper model '{model_name}'\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to load Whisper model.\")\n",
    "            raise\n",
    "\n",
    "    def transcribe(self, audio_path):\n",
    "        \"\"\"\n",
    "        Transcribe the audio file and return the transcript.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.model.transcribe(str(audio_path))\n",
    "            logger.info(\"Transcription successful.\")\n",
    "            return result.get(\"text\", \"\")\n",
    "        except Exception:\n",
    "            logger.exception(f\"Transcription failed for {audio_path}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9702d7",
   "metadata": {
    "papermill": {
     "duration": 0.027105,
     "end_time": "2025-04-20T04:06:12.710804",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.683699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Meeting Summarization (Gemini)\n",
    "\n",
    "This class summarizes meeting transcripts using Google's Gemini API.  \n",
    "It sends a structured prompt and expects the response in JSON format, which is then parsed for downstream use like slide generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05df6204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:12.766400Z",
     "iopub.status.busy": "2025-04-20T04:06:12.766112Z",
     "iopub.status.idle": "2025-04-20T04:06:12.772140Z",
     "shell.execute_reply": "2025-04-20T04:06:12.771319Z"
    },
    "papermill": {
     "duration": 0.035691,
     "end_time": "2025-04-20T04:06:12.773472",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.737781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeetingSummarizer:\n",
    "    \"\"\"\n",
    "    Summarizes meeting transcripts via Google GenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def summarize(self, transcript, prompt):\n",
    "        \"\"\"\n",
    "        Generates a structured summary (sections with titles, summary, bullets).\n",
    "        \"\"\"\n",
    "    \n",
    "        config = types.GenerateContentConfig(\n",
    "            max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "        )\n",
    "        try:\n",
    "            resp = self.client.models.generate_content(\n",
    "                model=MODEL_NAME, config=config, contents=[prompt, transcript]\n",
    "            )\n",
    "            summary_text = resp.text\n",
    "            json_str = summary_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            summary_slides = json.loads(json_str)\n",
    "            logger.info(\"Parsed summary to JSON.\")\n",
    "            return summary_slides, summary_text\n",
    "        except Exception:\n",
    "            logger.exception(\"Summarization error.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56731400",
   "metadata": {
    "papermill": {
     "duration": 0.027683,
     "end_time": "2025-04-20T04:06:12.829198",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.801515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation with Gemini (Rubric-Based)\n",
    "\n",
    "Here, a GenAI-based evaluator scores the generated summary for its presentation quality.\n",
    "\n",
    "The model is prompted with an evaluation rubric based on:\n",
    "- Slide structure\n",
    "- Groundedness\n",
    "- Conciseness\n",
    "- Fluency\n",
    "\n",
    "This enables an automated review loop that mimics human feedback for quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488657fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:12.885739Z",
     "iopub.status.busy": "2025-04-20T04:06:12.885448Z",
     "iopub.status.idle": "2025-04-20T04:06:12.892801Z",
     "shell.execute_reply": "2025-04-20T04:06:12.892157Z"
    },
    "papermill": {
     "duration": 0.037766,
     "end_time": "2025-04-20T04:06:12.894180",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.856414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SummaryRating(enum.Enum):\n",
    "    VERY_GOOD = '5'\n",
    "    GOOD = '4'\n",
    "    OK = '3'\n",
    "    BAD = '2'\n",
    "    VERY_BAD = '1'\n",
    "    \n",
    "class SummaryEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates Summary generated from transcript via Google GenAI.\n",
    "    \"\"\"\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def eval(self, summary, transcript, summary_prompt):\n",
    "        \"\"\"\n",
    "        Evaluate the summary of the transcript and provide a rating on a scale 1 to 5, 1 being \"Very Poor\" and 5 being \"Excellent\"\n",
    "        \"\"\"\n",
    "        prompt = (f\"\"\"\n",
    "            # Instruction\n",
    "            You are an expert evaluator for slide presentations. Your task is to evaluate the quality of a meeting summary generated for a transcript by an AI model, which is intended to be turned into a PowerPoint slide deck.\n",
    "            \n",
    "            We will provide you with the original prompt and transcript given to the model and the AI-generated structured summary. Your evaluation should focus on whether this summary is effective for slide-based presentation.\n",
    "            \n",
    "            # Evaluation\n",
    "            ## Metric Definition\n",
    "            You will assess the meeting summary’s quality with regard to slide-readiness. A good slide summary should:\n",
    "            - Break the content into clear sections\n",
    "            - Contain accurate and concise summaries\n",
    "            - Use bullet points that can be used directly in presentation slides\n",
    "            - Avoid introducing information that wasn't in the source\n",
    "            \n",
    "            ## Criteria\n",
    "            1. **Structure for Slides**: The summary is clearly broken down into presentation-friendly sections with meaningful titles.\n",
    "            2. **Groundedness**: The summary uses only content grounded in the original meeting transcript and does not hallucinate.\n",
    "            3. **Conciseness and Slide-Readiness**: The bullets are clear, well-chunked, and ready to be used on slides (not full paragraphs).\n",
    "            4. **Fluency and Readability**: The summaries and bullets are easy to understand and grammatically correct.\n",
    "            \n",
    "            ## Rating Rubric\n",
    "            5 (Excellent): Summary is well-structured, fully grounded, concise, and presentation-ready with fluent writing.\n",
    "            4 (Good): Summary is mostly well-structured and grounded; bullets are usable with minor edits.\n",
    "            3 (Fair): Summary is okay but needs editing to be usable in slides (e.g., too verbose, not well-structured).\n",
    "            2 (Poor): Summary is grounded but hard to use in a slide deck without major revisions.\n",
    "            1 (Very Poor): Summary is ungrounded, off-topic, or incoherent.\n",
    "            \n",
    "            ## Evaluation Steps\n",
    "            STEP 1: Assess the summary for presentation-readiness using the 4 criteria.\n",
    "            STEP 2: Score the summary using the rubric.\n",
    "            \n",
    "            # User Inputs and AI-generated Response\n",
    "            ## Prompt\n",
    "            {summary_prompt}\n",
    "            ## trabscript\n",
    "            {transcript}\n",
    "            \n",
    "            ## AI-generated Summary (JSON format intended for slide generation)\n",
    "            ```\n",
    "            {summary}\n",
    "            ```\n",
    "                \"\"\"\n",
    "        )\n",
    "        try:\n",
    "            resp = self.client.chats.create(\n",
    "                model=MODEL_NAME).send_message(prompt)\n",
    "            verbose_eval = resp.text\n",
    "            logger.info(\"Evaluated the summary generated.\")\n",
    "            return verbose_eval\n",
    "        except Exception:\n",
    "            logger.exception(\"Evaluation error.\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f844070",
   "metadata": {
    "papermill": {
     "duration": 0.027292,
     "end_time": "2025-04-20T04:06:12.949088",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.921796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Slide Generation (python-pptx)\n",
    "\n",
    "This section uses the `python-pptx` library to automatically convert the structured JSON summary \n",
    "into a clean and readable PowerPoint presentation. It includes:\n",
    "- Section titles\n",
    "- Summaries\n",
    "- Bullet points styled for readability\n",
    "\n",
    "The goal is to reduce manual effort in summarizing and preparing meeting notes into presentable slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9faf7eed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:13.006947Z",
     "iopub.status.busy": "2025-04-20T04:06:13.006114Z",
     "iopub.status.idle": "2025-04-20T04:06:13.019620Z",
     "shell.execute_reply": "2025-04-20T04:06:13.018976Z"
    },
    "papermill": {
     "duration": 0.044515,
     "end_time": "2025-04-20T04:06:13.021039",
     "exception": false,
     "start_time": "2025-04-20T04:06:12.976524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPTGenerator:\n",
    "    \"\"\"\n",
    "    Create a PowerPoint from structured summary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, themes):\n",
    "        self.themes = themes\n",
    "\n",
    "    @staticmethod\n",
    "    def hex_to_rgb(h):\n",
    "        h = h.lstrip(\"#\")\n",
    "        return tuple(int(h[i : i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "    @staticmethod\n",
    "    def is_dark(rgb):\n",
    "        r, g, b = rgb\n",
    "        return (0.299 * r + 0.587 * g + 0.114 * b) < 150\n",
    "\n",
    "    @staticmethod\n",
    "    def add_bg(slide, rgb):\n",
    "        shape = slide.shapes.add_shape(\n",
    "            MSO_SHAPE.RECTANGLE, Inches(0), Inches(0), Inches(10), Inches(7.5)\n",
    "        )\n",
    "        shape.fill.solid()\n",
    "        shape.fill.fore_color.rgb = RGBColor(*rgb)\n",
    "        shape.line.fill.background()\n",
    "        slide.shapes._spTree.insert(2, slide.shapes._spTree[-1])\n",
    "\n",
    "    def generate(self, sections, filename, mtype = \"default\"):\n",
    "        \"\"\"\n",
    "        Build and save the .pptx file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            theme = MEETING_THEMES.get(mtype, MEETING_THEMES[\"default\"])\n",
    "            rgb = self.hex_to_rgb(theme[\"color\"])\n",
    "            font_rgb = (255, 255, 255) if self.is_dark(rgb) else (0, 0, 0)\n",
    "            use_light_text = self.is_dark(rgb)\n",
    "            prs = Presentation()\n",
    "\n",
    "            # Title Slide\n",
    "            slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
    "            self.add_bg(slide, rgb)\n",
    "            slide.shapes.title.text = theme[\"title\"]\n",
    "            slide.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                *font_rgb\n",
    "            )\n",
    "            body = slide.placeholders[1]\n",
    "            body.text = filename.stem\n",
    "            body.text_frame.paragraphs[0].font.color.rgb = RGBColor(*font_rgb)\n",
    "\n",
    "            # TOC\n",
    "            toc = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "            self.add_bg(toc, rgb)\n",
    "            toc.shapes.title.text = \"Table of Contents\"\n",
    "            toc.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                *font_rgb\n",
    "            )\n",
    "            toc_body = toc.placeholders[1]\n",
    "            toc_body.text = \"\\n\".join(s[\"section_title\"] for s in sections)\n",
    "            for p in toc_body.text_frame.paragraphs:\n",
    "                p.font.color.rgb = RGBColor(*font_rgb)\n",
    "\n",
    "            # Content Slides\n",
    "            for sec in sections:\n",
    "                sld = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "                self.add_bg(sld, rgb)\n",
    "                sld.shapes.title.text = sec[\"section_title\"]\n",
    "                sld.shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(\n",
    "                    *font_rgb\n",
    "                )\n",
    "                box = sld.placeholders[1]\n",
    "                tf = box.text_frame\n",
    "                tf.clear()\n",
    "                summary_text = sec.get(\"summary\", \"\")\n",
    "                summary_pt = tf.add_paragraph()\n",
    "                summary_pt.text = f\"Summary: {summary_text}\"\n",
    "                summary_pt.font.size = Pt(18)\n",
    "                summary_pt.font.color.rgb = RGBColor(*font_rgb)\n",
    "                summary_pt.font.bold = True\n",
    "                for bullet in sec.get(\"bullets\", []):\n",
    "                    pb = tf.add_paragraph()\n",
    "                    pb.text = bullet\n",
    "                    pb.level = 1\n",
    "                    pb.font.size = Pt(20)\n",
    "                    pb.font.color.rgb = RGBColor(*font_rgb)\n",
    "                    \n",
    "\n",
    "            prs.save(str(filename))\n",
    "            logger.info(f\"PPT saved: {filename}\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed PPT generation.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171bdc3",
   "metadata": {
    "papermill": {
     "duration": 0.027224,
     "end_time": "2025-04-20T04:06:13.076141",
     "exception": false,
     "start_time": "2025-04-20T04:06:13.048917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding and Retrieval\n",
    "\n",
    "This part introduces **retrieval-augmented generation (RAG)** by embedding previous meeting transcripts \n",
    "and storing them in a vector database (`ChromaDB`). When a new meeting is processed, similar \n",
    "past meetings can be retrieved to provide context or track recurring themes.\n",
    "\n",
    "This enhances the agent’s memory and understanding of long-term patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb6fc83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:13.132260Z",
     "iopub.status.busy": "2025-04-20T04:06:13.131984Z",
     "iopub.status.idle": "2025-04-20T04:06:13.144829Z",
     "shell.execute_reply": "2025-04-20T04:06:13.143927Z"
    },
    "papermill": {
     "duration": 0.042843,
     "end_time": "2025-04-20T04:06:13.146145",
     "exception": false,
     "start_time": "2025-04-20T04:06:13.103302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAGEngine:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation using ChromaDB.\n",
    "    \"\"\"\n",
    "    def __init__(self, client, storage_path, collection_name):\n",
    "        self.client = client\n",
    "        self.storage_path = storage_path\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    class _EmbeddingFn(EmbeddingFunction):\n",
    "        def __init__(self, client, model):\n",
    "            self.client = client\n",
    "            self.model = model\n",
    "\n",
    "        def __call__(self, texts):\n",
    "            try:\n",
    "                res = self.client.models.embed_content(\n",
    "                    model=self.model,\n",
    "                    contents=texts,\n",
    "                    config=types.EmbedContentConfig(\n",
    "                    task_type=EMBEDDING_TASK,\n",
    "                    ),\n",
    "                )\n",
    "                raw = getattr(res, 'embeddings', None) or res.get('embeddings') or res.get('embedding')\n",
    "                if not raw:\n",
    "                    raise ValueError(\"No embeddings returned from GenAI.\")\n",
    "                processed = [item.values if hasattr(item, 'values') else item for item in raw]\n",
    "                return processed\n",
    "            except Exception:\n",
    "                logger.exception(\"Embedding failed.\")\n",
    "                return []\n",
    "\n",
    "    def init_db(self) :\n",
    "        \"\"\"\n",
    "        Initialize or get a ChromaDB collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client = chromadb.PersistentClient(path=str(self.storage_path))\n",
    "            existing = [col.name for col in client.list_collections()]\n",
    "            if self.collection_name in existing:\n",
    "                logger.info(\"Using existing ChromaDB collection.\")\n",
    "                return client.get_collection(name=self.collection_name)\n",
    "            logger.info(f\"Creating new ChromaDB collection: {self.collection_name}\")\n",
    "            return client.create_collection(\n",
    "                name=self.collection_name,\n",
    "                embedding_function=self._EmbeddingFn(self.client, EMBEDDING_MODEL)\n",
    "            )\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to initialize ChromaDB collection.\")\n",
    "            raise\n",
    "\n",
    "    def add_document(self, db, text):\n",
    "        \"\"\"\n",
    "        Add a document to ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if text:\n",
    "                db.add(documents=[text], ids=[str(uuid.uuid4())])\n",
    "                logger.info(\"Document added to ChromaDB.\")\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to add document to ChromaDB.\")\n",
    "\n",
    "    def query(self, db, query, k = 2):\n",
    "        \"\"\"\n",
    "        Query ChromaDB for top-k relevant documents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = db.query(query_texts=[query], n_results=k)\n",
    "            return result.get('documents', [])\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to query ChromaDB.\")\n",
    "            return []\n",
    "\n",
    "    def answer(self, db, query, k = 2):\n",
    "        \"\"\"\n",
    "        Answer a question using retrieved passages from ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            passages = self.query(db, query, k)\n",
    "            prompt = (\n",
    "                f\"\"\"You are a helpful and informative bot that answers questions using only the\n",
    "                provided reference passage below.\n",
    "                \n",
    "                Instructions:\n",
    "                - Provide a complete, well-explained answer based solely on the passage.\n",
    "                - If the answer is not available, respond with: \"I'm not sure.\"\n",
    "                - You are responding to a technical audience, so explain clearly but concisely.\n",
    "                - Break down complex concepts into understandable parts.\n",
    "                - Ignore irrelevant information.\n",
    "                \n",
    "                Passage:\n",
    "                {passages}\n",
    "                \n",
    "                Question:\n",
    "                {query}\n",
    "\n",
    "                Answer:RAG Q&A\"\"\"\n",
    "            )\n",
    "            resp = self.client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            return resp.text.strip()\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to generate answer.\")\n",
    "            return \"I'm not sure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35f2f1",
   "metadata": {
    "papermill": {
     "duration": 0.027595,
     "end_time": "2025-04-20T04:06:13.201239",
     "exception": false,
     "start_time": "2025-04-20T04:06:13.173644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## End-to-End Workflow: Transcribe → Summarize → Evaluate → Generate Slides → Retrieve → QA\n",
    "\n",
    "This function orchestrates the full **MeetingMate pipeline**:\n",
    "\n",
    "1. **Transcribe**: Converts raw audio to text using Whisper.\n",
    "2. **Summarize**: Uses Gemini API with structured prompting to generate a JSON summary.\n",
    "3. **Evaluate**: Scores the summary using a rubric-based GenAI evaluator.\n",
    "4. **Detect Theme**: Classifies the meeting type (e.g., retrospective, kickoff).\n",
    "5. **Generate Slides**: Converts the summary into a PowerPoint presentation.\n",
    "6. **RAG Q&A**: Embeds the transcript and performs retrieval-augmented question answering using ChromaDB.\n",
    "\n",
    "This design demonstrates a complete GenAI application combining multimodal processing, structured prompting, evaluation, generation, and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efedf1ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T04:06:13.258052Z",
     "iopub.status.busy": "2025-04-20T04:06:13.257430Z",
     "iopub.status.idle": "2025-04-20T04:07:10.801617Z",
     "shell.execute_reply": "2025-04-20T04:07:10.800889Z"
    },
    "papermill": {
     "duration": 57.574501,
     "end_time": "2025-04-20T04:07:10.803182",
     "exception": false,
     "start_time": "2025-04-20T04:06:13.228681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 134MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## **Structured output of the Summary Generated by the model**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "  {\n",
       "    \"section_title\": \"Discussion on Participant Recruitment and Incentives\",\n",
       "    \"summary\": \"The group discusses strategies for attracting participants to the meetings, including location, food, and incentives like providing transcripts and CDs of their speech.\",\n",
       "    \"bullets\": [\n",
       "      \"Holding meetings in a convenient and underused location is preferred over providing free lunch.\",\n",
       "      \"Offering participants a transcript of their speech is a good incentive, potentially followed by a CD after a screening phase.\",\n",
       "      \"Recruiting participants from diverse backgrounds, not just linguists or engineers, is important.\"\n",
       "    ]\n",
       "  },\n",
       "  {\n",
       "    \"section_title\": \"Disk Space Management and Archiving\",\n",
       "    \"summary\": \"The conversation shifts to the issue of limited disk space and the ongoing process of archiving files to free up space for recording more meetings.\",\n",
       "    \"bullets\": [\n",
       "      \"Archiving old files, particularly from broadcast news, is being done to create more space.\",\n",
       "      \"The archiving process is slow, but retrieving archived files is relatively quick.\",\n",
       "      \"Archived files are backed up with clones to ensure data safety.\"\n",
       "    ]\n",
       "  }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation of AI-Generated Meeting Summary\n",
       "\n",
       "**STEP 1: Assess the summary for presentation-readiness using the 4 criteria.**\n",
       "\n",
       "*   **Structure for Slides**: The summary is well-structured into two distinct sections with meaningful titles that accurately reflect the content of the transcript.\n",
       "*   **Groundedness**: The summary is entirely grounded in the original meeting transcript. No hallucinated information is present.\n",
       "*   **Conciseness and Slide-Readiness**: The bullet points are concise and directly usable on slides. They are well-chunked and avoid being overly verbose.\n",
       "*   **Fluency and Readability**: The summaries and bullet points are easy to understand and grammatically correct.\n",
       "\n",
       "**STEP 2: Score the summary using the rubric.**\n",
       "\n",
       "*   **Overall Score: 5 (Excellent)**\n",
       "\n",
       "**Justification:**\n",
       "\n",
       "The AI-generated summary is exceptionally well-suited for creating a slide deck. It effectively breaks down the transcript into logical sections with clear and concise titles. The summaries are accurate, and the bullet points are perfectly formatted for direct use on slides. The content is fully grounded in the transcript, and the writing is fluent and easy to understand. This summary requires virtually no editing to be transformed into a presentation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## **Meeting Type**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "team_sync"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## **RAG Q&A**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What's the main priority for getting people to the underused room?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:The first priority should be to try to get people to come to the underused room because they are already set up for it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What audience groups are excluded?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:The audience groups that are excluded are linguists and engineers. They want a wider sampling of people that are not linguists or engineers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q:What's the plan for distributing audio recordings or 'CDs' post-session?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Ans:Initially, there were concerns about immediately providing participants with CDs due to potential issues. The current plan involves providing CDs only after the transcripts have been screened. The CD would ideally be the same version that has been publicly reviewed, ensuring no unauthorized content is shared."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### **Done**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(audio_bytes):\n",
    "    \"\"\"\n",
    "    Workflow: transcribe -> summarize and RAG Q&A -> detect type -> ppt\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key = GOOGLE_API_KEY\n",
    "        gen_client = Client(api_key=key)\n",
    "\n",
    "        #### Transcription ####\n",
    "        transcriber = AudioTranscriber()\n",
    "        path = save_file(audio_bytes)\n",
    "        transcript = transcriber.transcribe(path)\n",
    "\n",
    "        #### Summarization ####\n",
    "        summary_prompt = (\"\"\"\n",
    "            You are a meeting assistant. Carefully analyze the meeting transcript and:\n",
    "                1. Segment it into distinct topics (whenever the conversation focus shifts).\n",
    "                2. For each topic:\n",
    "                  - Assign a meaningful short section_title (max 1 line).\n",
    "                  - Write a concise 1-sentence summary.\n",
    "                  - Extract 2-3 bullet points (each bullet under 50 words).\n",
    "                \n",
    "            Think step by step. Identify topic shifts chronologically.\n",
    "            Return the result strictly in the JSON format. All keys must be in double quotes:\n",
    "                ```\n",
    "                  {\n",
    "                    \"section_title\": \"Team Updates\",\n",
    "                    \"summary\": \"...\",\n",
    "                    \"bullets\": [\"...\", \"...\", \"...\"]\n",
    "                  },\n",
    "                  ...\n",
    "                ```\n",
    "                \"\"\"\n",
    "        )\n",
    "        summarizer = MeetingSummarizer(gen_client)\n",
    "        summary, summary_text = summarizer.summarize(transcript, summary_prompt)\n",
    "        display(Markdown(\"## **Structured output of the Summary Generated by the model**\"))\n",
    "        display(Markdown(summary_text))\n",
    "\n",
    "        #### Summary Evaluation ####\n",
    "        evaluate = SummaryEvaluator(gen_client)\n",
    "        evaluation = evaluate.eval(summary, transcript, summary_prompt)\n",
    "        display(Markdown(evaluation))\n",
    "        \n",
    "        #### Theme Detection ####\n",
    "        theme_prompt = (\"\"\"\n",
    "        You're a Meeting assistant. Given the context, understand it and choose the meeting type accordingly from the list:\n",
    "        [\"team_sync\", \"project_kickoff\", \"client_review\", \"retrospective\"].\n",
    "                \n",
    "        Reply with just the meeting type.\n",
    "        \"\"\"\n",
    "        )\n",
    "        theme_resp = gen_client.models.generate_content(\n",
    "            model=MODEL_NAME, contents=[theme_prompt, transcript]\n",
    "        )\n",
    "        mtype = theme_resp.text.strip().lower()\n",
    "        display(Markdown(\"## **Meeting Type**\"))\n",
    "        display(Markdown(mtype))\n",
    "\n",
    "        #### PPT generation ####\n",
    "        ppt = PPTGenerator(MEETING_THEMES)\n",
    "        ppt.generate(summary, Path(PPTX_FILENAME), mtype)\n",
    "\n",
    "        #### RAG Q&A using Chroma ####\n",
    "        rag = RAGEngine(gen_client, CHROMA_STORAGE_PATH, COLLECTION_NAME)\n",
    "        db = rag.init_db()\n",
    "        rag.add_document(db, transcript)\n",
    "\n",
    "\n",
    "        # Example QA\n",
    "        questions = [\n",
    "            \"What's the main priority for getting people to the underused room?\",\n",
    "            \"What audience groups are excluded?\",\n",
    "            \"What's the plan for distributing audio recordings or 'CDs' post-session?\",\n",
    "        ]\n",
    "\n",
    "        display(Markdown(\"## **RAG Q&A**\"))\n",
    "        for question in questions:\n",
    "            display(Markdown(\"Q:\"+question))\n",
    "            ans = rag.answer(db, question, k=2)\n",
    "            display(Markdown(\"Ans:\"+ans))\n",
    "\n",
    "    except Exception:\n",
    "        logger.exception(\"Main workflow error.\")\n",
    "        raise\n",
    "    display(Markdown(\"#### **Done**\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_bytes = src_path.read_bytes()\n",
    "    main(audio_bytes)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7195486,
     "sourceId": 11480462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 199.06096,
   "end_time": "2025-04-20T04:07:12.557689",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T04:03:53.496729",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
